# -*- coding: utf-8 -*-
"""CNIT Assignment #5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WdFacXZdhieHVm2usXwqXrreNT8zTdpV

**Retrieving the datasets**

getting the MNIST dataset, code copied directly from the assignments
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import keras as k
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.optimizers import SGD, Adam
from keras.models import load_model
from keras import backend as K

"""fetching and downloading the other datasets"""

import requests
import zipfile
import os

def download_file(url, filename):
    response = requests.get(url)
    if response.status_code == 200:
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded: {filename}")
    else:
        print(f"Failed to download {url}: Status code {response.status_code}")

# Example URLs (replace with your actual URLs)
urls = {
    "MSRA-TD500": "http://www.iapr-tc11.org/dataset/MSRA-TD500/MSRA-TD500.zip",
    "CROHME": "http://www.iapr-tc11.org/dataset/CROHME/CROHME_full_v2.zip"
}

for name, url in urls.items():
    download_file(url, f"{name}.zip")


def extract_zip(filename, extract_to='.'):
    if not os.path.exists(filename):
        print(f"{filename} does not exist.")
        return

    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"Extracted: {filename} to {extract_to}")


# Extract the downloaded files
zip_files = ["MSRA-TD500.zip", "CROHME.zip"]
for zip_file in zip_files:
    extract_zip(zip_file, extract_to='./extracted_files')

"""**Designing a unified model**

Preprocessing and labelling (some code copied from the previous assignments)
"""

import cv2  # OpenCV for image loading
import numpy as np
# For standalone Keras
from keras.utils import to_categorical

# For TensorFlow Keras
from tensorflow.keras.utils import to_categorical


### MNIST
(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()
img_rows, img_cols = 28, 28
x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], img_rows, img_cols, 1)
x_test_mnist = x_test_mnist.reshape(x_test_mnist.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)
x_test_mnist = x_test_mnist.astype('float32')
x_train_mnist = x_train_mnist.astype('float32')
mean = np.mean(x_train_mnist)
std = np.std(x_train_mnist)
x_test_mnist = (x_test_mnist - mean) / std
x_train_mnist = (x_train_mnist - mean) / std

num_classes = 10
y_train_mnist = k.to_categorical(y_train_mnist, num_classes)
y_test_mnist = k.to_categorical(y_test_mnist, num_classes)

def load_images_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if os.path.isfile(img_path):
            # Load image
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                img = cv2.resize(img, (img_rows, img_cols))  # Resize to match input shape
                images.append(img)
                label = "images/expressions"
                labels.append(label)
    return np.array(images), np.array(labels)

### Loading MSRA-TD500
msra_folder = './extracted_files/MSRA-TD500'
x_train_msra, y_train_msra = load_images_from_folder(msra_folder)

x_train_msra = x_train_msra.reshape(x_train_msra.shape[0], img_rows, img_cols, 1)
x_train_msra = x_train_msra.astype('float32')
x_train_msra = (x_train_msra - mean) / std

y_train_msra = k.to_categorical(y_train_msra, num_classes)

### Loading CROHME
crohme_folder = '/content/extracted_files/CROHME2013_data'
x_test_crohme, y_test_crohme = load_images_from_folder(crohme_folder)

x_test_crohme = x_test_crohme.reshape(x_test_crohme.shape[0], img_rows, img_cols, 1)
x_test_crohme = x_test_crohme.astype('float32')
x_test_crohme = (x_test_crohme - mean) / std

y_test_crohme = k.to_categorical(y_test_crohme, num_classes)

### Combine Datasets
x_train_combined = np.concatenate((x_train_mnist, x_train_msra), axis=0)
y_train_combined = np.concatenate((y_train_mnist, y_train_msra), axis=0)

x_test_combined = np.concatenate((x_test_mnist, x_test_crohme), axis=0)
y_test_combined = np.concatenate((y_test_mnist, y_test_crohme), axis=0)

"""The actual model; copied code from assignment 2"""

#build model

num_filter=32
num_dense=512
drop_dense=0.7
ac='relu'
learningrate=0.001

model = Sequential()

model.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32

model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons

model.add(Flatten())
model.add(Dense(num_dense, activation=ac))
model.add(BatchNormalization())
model.add(Dropout(drop_dense))
model.add(Dense(10, activation='softmax'))

adm=Adam(lr=learningrate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)

with tf.device("/cpu:0"):
    model = Sequential()

    model.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32

    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons

    model.add(Flatten())
    model.add(Dense(num_dense, activation=ac))
    model.add(BatchNormalization())
    model.add(Dropout(drop_dense))
    model.add(Dense(10, activation='softmax'))

    adm=Adam(lr=learningrate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=adm)

with tf.device("/cpu:0"):
    for i in range(5):
        k=8*2**i
        print("batch size "+str(k))
        model.fit(x_train_combined, y_train_combined, batch_size=k, epochs=1, validation_data=(x_test_combined, y_test_combined))