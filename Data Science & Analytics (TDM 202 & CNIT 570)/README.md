# The Data Mine
Work I did during my time at the Purdue Data Mine. The Data Mine is a learning community where students of any major can learn data science concepts and apply them to their projects. This is extremely beneficial for me because it was the first time I could use the Python libraries I learned to good use and get practice with them. For me, it offered real-life coding experience which isn't available in-class. the focus wasn't learning concepts, but applying what you know and what you learned

# Experience Acquired
+ Represented, extracted, manipulated, interpreted, transformed, and visualized big data sets. Moreover, I explored, analyzed, and communicated insights about the data sets.
+ Working with Tensorflow, PySpark (along with Spark SQL and Apache Spark), MLib, GraphX, Extensible Markup Language (XML)
+ Scraping and parsing websites using Selenium and BeautifulSoup
+ Gathering data using different methods (XPath, By.CSS_SELECTOR, etc)
+ Using PANDAS in a real-world setting: reading CSV files, exploring them and understanding what they're about, and working with them
+ Using sklearn and tensorflow to make, compile, and train a model
+ Cleaning dataframes using PANDAS, plotting using Plotly to visualize trends in data, and removing rows, columns, and specific entries to get a better understanding of trends

# Data Analytics
+ This notebook provides an excellent introduction for someone unfamiliar with Random Forests or LSTMs, as it includes practical implementations and clear visualizations of their performance. By following the code, you can learn how to prepare data for both models, which is a crucial first step in machine learning. For Random Forests, you'll see how they handle structured data effectively without much preprocessing, making them a great starting point for understanding ensemble methods. You’ll also observe how to tune parameters like the number of trees to optimize performance. For LSTMs, the notebook introduces time series analysis, showing how to structure sequential data and set up layers for the model. This offers insight into deep learning techniques tailored for temporal data.
+ To properly use these models, it's essential to understand their strengths and limitations. Random Forests are robust for tabular data and can highlight feature importance, but they may struggle with sequential dependencies. On the other hand, LSTMs excel at capturing temporal patterns but require more careful data preparation, such as normalizing and structuring the input sequences. This notebook demonstrates these differences, teaching you to choose the right model for the task and how to interpret results like loss and training time. By experimenting with the provided code, you’ll gain hands-on experience that builds foundational knowledge for using these models in real-world applications.
